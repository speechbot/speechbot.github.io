<!DOCTYPE html>
<html>
    <head>
        <meta charset="UTF-8">
        <title>textlss NLP project</title>                      
        <link rel="stylesheet" type="text/css" href="styles.css"> 
        <script src="jquery-3.5.js"></script>
    </head>
    <body>
		<div class="container">
		    <h1 id="text1">The Textlss NLP project</h1>
		    <div id="intro">
		        <br>
		    </div>
		</div>

	    <div class="content-container">            
	    	<h2>The Research</h2>
	        <p>
	            <b>Language models</b> trained from large corpora of text have made tremendous progress in the recent years, and are used in a variety of Natural Language Processing (NLP) applications. Independently, recent breakthrough in <b>representation learning</b> has yielded models able to discover discrete units from raw audio without the need of any labeled data. Connecting these two breakthroughs together opens up the possibility of applying language models directly to audio inputs, side stepping the need for textual resources or Automatic Speech Recognition (ASR), opening up a new era of <b>textless NLP</b>. This may seem an unachievable objective, but preschool children provide a proof of principle that it is possible to master a great deal about language from raw sensory inputs and interactions only, without any text. Inspired by this amazing learning ability, we introduce our first baby steps in this exciting new area. 
	        </p>	        
	        <br/>        
	    </div>
	    <div class="content-container">            
	    	<h2>Milestones</h2>
	        <ol>
	        	<li>Generative Spoken Language Modeling from Raw Audio (GSLM): [<a href="gslm/index.html" target="_blank">demo</a>, <a href="https://arxiv.org/pdf/2102.01192.pdf" target="_blank">paper</a>, <a href="https://github.com/pytorch/fairseq/tree/master/examples/textless_nlp/gslm" target="_blank">code</a>].</li>
	        	<li>Speech Resynthesis from Discrete Disentangled Self-Supervised Representations: [<a href="resynthesis/index.html" target="_blank">demo</a>, <a href="https://arxiv.org/pdf/2104.00355.pdf" target="_blank">paper</a>, <a href="" target="_blank">code</a>].</li>
	        	<li>Expressive Generative Spoken Language Modeling: [<a href="" target="_blank">demo</a>, <a href="" target="_blank">paper</a>, <a href="" target="_blank">code</a>].</li>
	        </ol>
	        <!-- <div class="row">
	            <div class='col'>
	                <p class="text-left">Generative Spoken Language Modeling from Raw Audio (GSLM)</p>
	                <a href="gslm/index.html" class="thumbnail" target="blank">
	                     <img src="img/gslm.png">
	                </a>
	            </div>
	            <div class='col'>
	                <p class="text-left">Speech Resynthesis from Discrete Disentangled Self-Supervised Representations</p>
	                <a href="resynthesis/index.html" class="thumbnail" target="blank">
	                     <img src="img/gslm2.png">
	                </a>
	            </div>
	            <div class='col'>
	                <p class="text-left">Expressive Generative Spoken Language Modeling</p>
	                <a href="" class="thumbnail" target="blank">
	                </a>
	            </div>          
          	</div> -->
	    </div>

	    <div class="content-container">            
	    	<h2>Why it matters</h2>
Despite their growing range of applications, NLP technologies are limited in their scope by the availability of massive quantities of text, which can only be achieved for a handful of economically dominant languages. This leaves out the majority of the world’s languages, which have little such resource. Being able to achieve ‘textless NLP’ would make AI applications <b>more inclusive</b>. Second, even for resource-rich languages, the oral language carries a lot of nuances, intonations (irony, anger, uncertaintly, etc) and expressive vocalizations (laughter, yawning, mouth clicks, etc) that are not captured by text . Modeling language directly from the audio have the potential of making AI applications <b>more natural and expressive</b>. Third, while text is still the dominant form of language on the web, a growing amount of audio-based contents like podcasts, local radios, social audio apps, on-line video games open up a large vista of <b>audio-first experiences</b> to be built on top of such contents without needing to annotate them to train an ASR. 
	    </div>

	      
	    <div class="content-container">            
	    	<h2>The Team</h2>
This represents the work of a multi-disciplinary team of researchers with expertise in signal processing, speech processing, natural language processing and psycholinguistics from Facebook AI Research, Paris, Tel Aviv, New York and Seattle.
	    </div>

	      
	    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
	</body>
</html>
